\chapter{Navigation Systems Analysis}
There is a number of interesting papers in the field of navigation systems for visually impaired. Generally speaking, there are ongoing efforts to create maps for indoor environments, with the Google Indoor Maps being the head of this movement. Currently, the Google Indoor Maps are in beta and are not a priori intended for navigation but merely to provide the user with an approximate idea of where they are. In this chapter we will analyze some of the existing works which specifically address the problem of indoor navigation and focus on projects that specialize in navigation of visually impaired.\\
There are two main approaches to the problem. In the first, the navigation system consists of active parts which, using triangulation or other methods, are able to determine the user's position at all times and then give her directions based on knowing where she is. In the second approach, the system does not possess the information about user's position at all times. Instead it synchronizes the position at the beginning of the navigation task and then gives the user directions broken into small chunks. When the user believes she reached the destination described by the first chunk, she asks for the next one and etc. The disadvantage of this approach is that the user can get lost and not end up at the expected location. This problem can be solved by adding more "synchronization points" to strategic locations of the building.

\section{Current Systems for Visually Impaired}
NaviTerier \cite{naviterier} is a research project at FEE CTU which aims directly at the problem of navigating visually impaired inside buildings. This system does not require any specialized technical equipment. It relies only on mobile phones with voice output, which visually impaired people already use. The navigation system works on a principle of sequential presentation of carefully prepared description of the building to visually impaired user by the current mobile phone voice output. This system does not keep track of the user location. Instead, it breaks the directions into smaller pieces and then sequentially gives the  pieces to the user who follows them and asks for next portion when ready. This system was tested with 13 visually impaired users.\\
Recently this system was combined with UI Protocol platform, which is another research project of FEE CTU developed for the purpose of creating user interfaces customized to abilities and preferences of individual users. The result is navigational system called NaviTerier UIP (NUIP) \cite{balata} which combines the navigational part of NaviTerier and the ability of UIP to generate and deliver customized user interfaces that can fit better people with disabilities.\\

Luis et al.\cite{luis} propose a system which uses a infrared transmitter attached to the white cane combined with Wiimote units (the device of the Wii game console) placed so that they can determine the user's cane position using triangulation. The information from Wiimote units is communicated via Bluetooth to a computer which computes the position and then sends the directions to the user's smartphone via wifi. TTS engine running on the phone converts the directions to speech. The system has undergone preliminary testing with five blindfolded users. \\

An indoor navigation system to support the visually impaired is presented in \cite{riehle}. The paper describes creation of a system that utilizes a commercial Ultra-Wideband (UWB) asset tracking system to support real-time location and navigation information. The paper claims that the advantage of using UWB is its resistance to narrowband interference and its  robustness in complex indoor multipath environments. The system finds user position using triangulation and consists of four parts: tracking tag to be worn by the user, sensors that sense the position of the tracking tag, handheld navigator and a server which calculates the location of the tracking tag and communicates it to the navigator. The handheld device runs software which can produce audio directions to the user. In tests, the system proved useful; It was, however, tested only on blindfolded people.\\

Treuillet and Royer \cite{sylvie} proposed a vision-based localization for blind pedestrian navigation assistance in both outdoors and indoors. The solution uses a real-time algorithm to match particular references extracted from pictures taken by a body-mounted camera which periodically takes pictures of the surroundings. The extraction uses 3D landmarks which the system first has to learn by going through a path along which the user later want to navigate. It follows that the system is not suitable in environments that are visited for the first time. For the case when it has learned the way, the system performs well.

A promising approach is shown in the PERCEPT \cite{percept} project. Its architecture consists of the three system components: the Environment, the PERCEPT glove and Android client, and
the PERCEPT server. In the environment there are passive (i.e. no power supply needed) RFID tags (R-tags) deployed at strategic locations in a defined height and accompanied with signage of high contrast letters and embossed Braille. The next part of the environment are the Kiosks. Kiosks are where the user tells the system her destination. They are located at key locations of the building, such as elevators, entrances and exits and more. The R-tags are present here and the user has to find the one she needs and scan it using the glove.
The glove is used to scan the R-tags and also has buttons on it that the user can press to get the instructions for the part of the route, repeat previous instructions and get instructions back to the kiosk. Also, after scanning the R-tag the gloves sends its information to the app running on user's Android phone. The app connects to the internet and downloads the directions from the PERCEPT server. These are then presented to the user through a text-to-speech engine and the user follows them. The system was tested with 24 visually impaired users.
Another example of RFID use is presented in Lopez et al. \cite{lopez} where user is navigated by following paths marked by RFID labels on the floor. The white cane acts as an RFID reader and communicates with a smartphone which, as in other projects, uses TTS to give directions.
\\

There are also research works in the fields of robotics and artificial intelligence that study the problem of navigation. More specifically, they tackle the problem of real time indoor scenes recognition \cite{espinace}, \cite{quattoni}, \cite{bosch}. Some of these solutions allow for creating the reference map dynamically. Even though they proved to be useful in the domain of robotics and automotive industry, their applications to navigating people are limited, as they require expensive sensors and powerful computing resources. Wearing these devices would make the traveling of the users more difficult and limited. For these reasons, the solution proposed by Hesch and Roumeliotis \cite{hesch} is interesting because they integrated these devices (apart from the computing) into a white cane. However, the solution has the limitations in being too heavy and large, and the laser scanner being directional.



%sighted people
